---
title: "WNPRC_2018_data"
author: "Laura Abondano"
date: "2/12/2019"
output: html_document
---
#General Info
This script provides the necessary code to calculate hormone concentrations from samples that were analyzed using and enzyme immunoassay. The script uses the Optical Density (OD) values that are exported from a plate reader software, and then it uses those values to calculate the hormone concentrations in each sample. 

#First steps: import, reorganize samples and values, calculate mean and CV of duplicate values

The first step is to import your data. The best way of doing this is using the original text file that is exported from SoftmaxPro and asking R to read only the 8 rows that have the actual OD. We need to ignore all the text that is in the first part of the exported file. To do this we ask the function *skip* argument in the read.table() function to skip the first rows of the text file that have information that is not relevant for our analyses, and instead have it only read the 8 rows that we will be using in our analyses. The read.table() function imports the data as a data frame. 

I'm going to start importing the first plate ran in 2018, using this as an example to explain in detail the process of analyzing the data. Then I will proceed to run all of the other plates using the same code. 

```{r}
e1g.file.name<-'~/Box Sync/UT Box Sync/UT-Austin/DISSERTATION/Dissertation_R/Hormones_R/Diss_Hormones_R/WNPRCSoftmax2018/E1G - Woolly 20181101 PoolsPlate.txt'

# May need to modify the skip value according to the exported output from SoftmaxPro
data<-read.table(e1g.file.name, skip = 20, header = FALSE, nrows = 8) 

# If above code doesn't work, try specifying the fileEncoding (uncomment the following line of code):
#data<-read.table(e1g.file.name, skip = 19, header = TRUE, nrows = 8, fileEncoding = "UCS-2LE") 
```

Loading the required packages to be used. 

```{r}
library(dplyr)
library(ggplot2)
library(minpack.lm) # for non-linear models
```

Then we break the data up into one long vector (it reads the data by column, not by row), and then get the names back together according to how the plate was run (enter names by column, not by row). Then we put the IDs and the values together in a data frame and sort the data frame by the ID names.

```{r}
data<-unlist(data,use.names=F) 

ids<-c(rep(c("N","N","U13","U23","U01","Z01","U14","U24","U02","S01","U15","U25","U03","S02","U16","U26","U04","S03","U17","U27","U05","S04","C01","U28","U06","S05","C02","U29","U07","S06","U18","U30","U08","S07","U19","U31","U09","S08","U20","U32","U10","Z02","U21","U33","U11","U12","U22","U34"),each=2))

#put the IDs and the data together
work<-data.frame(ids,cod=data)

#sort data frame by ids
work<-work[order(ids),]
```

We are now going to replace the "generic" names with the actual sample names. I leave the controls (pools), zeros (N), and standards, the same, and just replace the name of the actual sample names. Also be careful to modify the number of standards depending on the type of assay you are running (i.e., E1G uses 8 standards vs. PdG that uses only 6 standards). 

```{r}
sample.name<-c(rep(c("C01","C02","N","N","S01","S02","S03","S04","S05","S06","S07","S08",
"WNPRC-2018-001",
"WNPRC-2018-006",
"WNPRC-2018-050",
"WNPRC-2018-107",
"WNPRC-2018-141",
"WNPRC-2018-149",
"WNPRC-2018-318",
"WNPRC-2018-320",
"WNPRC-2018-337",
"WNPRC-2018-339",
"WNPRC-2018-342",
"WNPRC-2018-348",
"WNPRC-2018-403",
"WNPRC-2018-409",
"WNPRC-2018-410",
"WNPRC-2018-418",
"WNPRC-2018-450",
"WNPRC-2018-028",
"WNPRC-2018-038",
"WNPRC-2018-042",
"WNPRC-2018-378",
"WNPRC-2018-394",
"WNPRC-2018-411",
"WNPRC-2018-414",
"WNPRC-2018-416",
"WNPRC-2018-419",
"WNPRC-2018-425",
"WNPRC-2018-430",
"WNPRC-2018-431",
"WNPRC-2018-438",
"WNPRC-2018-439",
"WNPRC-2018-446",
"WNPRC-2018-447",
"WNPRC-2018-449",
"Z01","Z02"),each=2))

# Add sample names to data frame
work<-cbind(work,sample.name)
work<-work[,c("ids","sample.name","cod")]
work
```

Since we ran samples in duplicate, the next step is to calculate the mean of each sample and the coefficient of variation for the duplicates. Samples that have a CV > 10 indicate that the mean is not a good reliable measurement for that sample, and therefore the sample should be assayed again. 

```{r}
# Calculating the mean
work2 <-
  group_by(work,sample.name)%>%
  summarise(meanod=mean(cod),
            stdev = sd(cod)) %>%
  mutate(cv=(stdev/meanod)*100)
```

We are then going to select out the blanks (N) to subtract the mean blank value from all samples. We are then going to select out the maximum binding values, or zero values (Z), to calculate the *percent binding* for each sample.  

```{r}
# select out blank 
blank<-unlist(select(filter(work2,sample.name == "N"),meanod),use.names=F)
blank

#subtract the blank from all wells gives you netod
work3<-mutate(work2, netod = meanod - blank)

# select out your maximum binding values (labelled as Z) 
BO<-unlist(select(filter(work3,sample.name %in% c("Z01","Z02")),netod),use.names=F) %>% mean(.)

# Calculate percent binding
work4<-mutate(work3,bound=(netod/BO)*100) 

```


# Making the standard curve and estimating sample values (pg/tube)

The first step to making the standard curve is to filter the standard values from the table, and then we need to use the estimated parameters for the *4-parameter logistic curve* (values taken from SoftmaxPro) to make our standard curve. The parameters used in this non-linear fit are the following: A = max assmptote, B = Slope factor, C = inflection point, D = min assymptote. The standard curve is needed to then plot the sample percent binging values on the standard curve to estimate the concentrations of the samples, using the known concentrations of each standard as a reference. 

**Not sure whether I should calculate a mean value for the parameters that were estimated across assays, or whether to use individual estimated parameters for each particular assay ran. Also, no estimates for the test plate, which was read using the old computer.**

```{r}
# Filter out standards to make standard curve
std.curve<-
  filter(work4,sample.name %in% c("S01","S02","S03","S04","S05","S06","S07","S08")) %>%
  select(sample.name,netod) %>%
  mutate(econ = (c(1.953,3.906,7.813,15.625,31.25,62.5,125,250)))


std.plot<-ggplot(std.curve, aes(x=econ,y=netod)) + coord_trans(x="log") + geom_point(data=std.curve,size=4,col='red')

#Make the non-linear fit
#Need to provide some estimates for A,B,C,D (estimates taken from Softmax Pro 7.0): (a=1.036,b=1.733,cc=20.47,d=0.074)

model=nlsLM(netod ~ d+(a-d)/(1+(econ/cc)^b),start=list(a=1.1,b=1.733,cc=20.47,d=0.02),data=std.curve) 
model


# plot the non-linear model 
min1<-min(std.curve$econ)
max1<-max(std.curve$econ)
model.pred<-data.frame(netod=predict(model,newdata=data.frame(econ=seq(min1,max1,by=.1))),econ=seq(min1,max1,by=.1))
fit<-1-(deviance(model)/sum((std.curve$netod-mean(std.curve$netod))^2))

final.plot<-std.plot +geom_line(data=model.pred, aes(x=econ,y=netod))
final.plot

```

Now we are going to calculate the sample concentrations based on where the percent bound values of each sample fall within the standard curve. Standard values are in pg/tube, and therefore the sample concentration values estimated from the standard curve will also be in pg/tb. 

**For those sample values that do not fit in the standard curve, the current code cannot extrapolate and therefore concentrations are left as NAs. To solve this I ran new assays using a much smaller sample volume/concentration, but that's something I will analyze later on.**

```{r}
#just a helper function does the opposite of '%in%
'%!in%' <- function(x,y)!('%in%'(x,y))

# To find pg/tube based on the standard curve:
# In this chunk of code we are selecting all samples that ARE NOT the standards, control pools, or zeros. 
final.nonlinear.samples<-
  filter(work4,sample.name %!in% c("N","S01","S02","S03","S04","S05","S06","S07","S08","C01","C02","Z01","Z02")) %>% mutate(pg.per.tube = coef(model)["cc"]*(((-1*coef(model)["a"]+netod)/(coef(model)["d"]-netod))^(1/coef(model)["b"])))
```

In some instances, there might be some sample concentrations that do not fit the standard curve (values are either higher or lower than the standard values), so the following chunk of code is useful to identify those samples that need to be re-assayed. 

```{r}
#Print samples that require re-runs:
rerun<-filter(final.nonlinear.samples,pg.per.tube %in% c("NaN"))
rerun
```


To replace very high or very low concentration values with maximum or minimum standard value (e.g., 250pg/tb as the maximum std value for E1G), use the following code chunk. However, this should ONLY be used for visualization purposes and not be considered as actual data. In fact, whenever possible, do not run this code but instead repeat the assay for those particular samples that fell outside the standard curve. 

```{r}
#Only uncomment following lines of code if necessary (but not recommended):
#final.nonlinear.samples[is.na(final.nonlinear.samples)] <- 1000 #really any number > 250 (max std val)
#final.nonlinear.samples$pg.per.tube[final.nonlinear.samples$pg.per.tube>250]<-250
```


#Calculating sample concentration values and expressing them in ng/g

Up to this point, we have our sample values as pg/tb, but we need to convert them into ng/g. To do this, we need to take into account the different dilutions that were used in the process. 

Given that the samples and the standards were treated equally (i.e., reconstituted with 225uL of EIA buffer, added 300uL of HRP and then plated 200uL), then it was not necessary to account for the dilution that takes into the account the percentage of the initial volume used in the assay that actually goes into each well (e.g., only 35.6% of the initial volume used for the assay goes into each well).

_Just in case... Where is this 35.6% coming from?_
_For example, for PdG we used 5uL of the original eluted sample and mixed that with 225uL of EIA buffer, but transfered only 200uL into the assay tubes, meaning that only 4.44uL ((5uL/225uL)x200uL) of the original 5uL was actually transfered into the assay tubes. In the assay tubes, the 4.44uL of sample was mixed with 500uL of EIA+HRP buffer, but only transfered 200uL to the plate to be read afterwards by the plate reader, meaning that only 1.78uL of the sample was plated in the EIA plate ((4.44uL/500uL)x200uL). Given that we ended up plating only 1.78uL of the sample, each well had 35.6% of the original 5uL of the eluted sample used in the assay._    

So there are two main *dilutions* that we need to take into account when calculating the actual concentration of hormones per gram of sample:

*1st dilution.*
The proportion of sample used from the initial solvent where feces were mixed in the extraction process, which was then pushed through the SPE column. At Tiputini, for example, from the 5mL used for extracting hormones into the solvent, only 2mL were pushed through the SPE column). In this particular example,  I used 40% of the original sample, so to back calculate to the amount of sample used in the total 5mL I need to multiply my result by 2.5. Therefore, my first dilution factor is of x2.5.

*2nd dilution.* 
Based on the amount of eluted sample used in each assay (e.g., 5uL for PdG, 5-15uL for E1G), and on the original volume of sample after elution and prior to assay (i.e., in 2017 samples were eluted in 2mL of methanol, dried, and then reconstituted in 1mL before running EIAs; while in 2018 samples were eluted in 2mL of methanol with no further processing before running assays). So for example, for PdG we used 5uL, therefore, in 2017 we used only 0.5% of the original 1mL of eluted sample, meaning that the dilution factor was x200, while in 2018 those 5uL represented 0.25% of the original 2mL of eluted sample, meaning that the dilution factor was x400.

```{r}
#Required data:
vol1<-5 #enter the volume used in the feces-solvent mix (in mL)
vol2<-2 #enter the amount of volume pushed through SPE (in mL)
vol3<-2000 #enter the volume in which sample was eluted/reconstituted (in uL)
vol4<-15 #enter the volume used in assay (in uL)


# Dilutions:
dil1<-vol1/vol2 # Dilution factor for percent of sample used from the original 5ml of solvent mixed with feces
dil2<-vol3/vol4 # Dilution factor to account for the volume of sample used in assay 

# Convert to ng/g:
ng.in.samples<- 
  filter(final.nonlinear.samples,sample.name %!in% c("C01","C02","S01","S02","S03","S04","S05","S06","S07","S08","Z01","Z02")) %>% 
  mutate(ng.per.sample = (pg.per.tube*dil1*dil2)/1000) #dividing by 1000 to convert it from pg to ng

# still need to do the math to calculate pool concentrations (different dilutions)

```



